# ğŸ“˜ Medical Diagnosis with LLMs: Evaluating Prompt Strategies on Gemini 2.0 Flash

This project evaluates seven prompting strategies for clinical diagnosis tasks using Googleâ€™s Gemini 2.0 Flash model.  
The goal is to measure **diagnostic accuracy**, **hallucination rate**, and **reasoning stability** across diverse prompting techniques on a curated medical MCQ dataset.

This project is fully reproducible and designed for **conference publication**.

---

## ğŸš€ Project Overview

Large Language Models (LLMs) can support clinical reasoning, but their reliability depends heavily on prompt design.  
This study systematically evaluates seven prompting strategies using a unified dataset and a reproducible experiment pipeline.

### Prompt strategies evaluated:
1. Zero-Shot  
2. Instruction-Heavy  
3. Few-Shot  
4. Chain-of-Thought (CoT)  
5. Self-Consistency (3 samples)  
6. Self-Verification  
7. Safety-Constrained Prompt  

The experiments compute:
- **Accuracy**
- **Hallucination Rate**
- **Prompt stability**
- **Multi-sample reasoning (self-consistency)**

---

## ğŸ“‚ Project Structure

PAPER-2/
â”‚
â”œâ”€â”€ analysis/
â”‚ â”œâ”€â”€ analysis.py
â”‚ â”œâ”€â”€ prompt_performance.csv
â”‚ â”œâ”€â”€ summary.txt
â”‚ â”œâ”€â”€ visualize.py
â”‚ â””â”€â”€ charts/
â”‚ â”œâ”€â”€ accuracy.png
â”‚ â””â”€â”€ hallucination.png
â”‚
â”œâ”€â”€ dataset/
â”‚ â”œâ”€â”€ medical_dataset.csv
â”‚ â””â”€â”€ medical_dataset.json
â”‚
â”œâ”€â”€ prompts/
â”‚ â””â”€â”€ prompts.txt
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ *.csv (one file per prompt type)
â”‚
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ run_experiment.py
â”‚
â”œâ”€â”€ paper/
â”‚ â”œâ”€â”€ sections/
â”‚ â”‚ â”œâ”€â”€ abstract.md
â”‚ â”‚ â”œâ”€â”€ introduction.md
â”‚ â”‚ â”œâ”€â”€ methods.md
â”‚ â”‚ â”œâ”€â”€ experiments.md
â”‚ â”‚ â”œâ”€â”€ results.md
â”‚ â”‚ â”œâ”€â”€ discussion.md
â”‚ â”‚ â””â”€â”€ conclusion.md
â”‚ â”œâ”€â”€ references.bib
â”‚ â””â”€â”€ figures/
â”‚ â””â”€â”€ pipeline_diagram.png
â”‚
â”œâ”€â”€ venv/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

markdown
Copy code

---

## ğŸ“Š Evaluation Pipeline

The entire project is powered by the following components:

### âœ” **Dataset Loader**
Reads medical MCQs from CSV format.

### âœ” **Prompt Loader**
Loads all seven prompting templates from a unified `prompts.txt`.

### âœ” **Experiment Engine**
`run_experiment.py` automates:
- Prompt formatting  
- API calls  
- Multi-API key rotation  
- Error handling  
- Self-consistency sampling (majority voting)  
- Answer extraction  
- Hallucination detection  
- CSV export  

### âœ” **Analysis Script**
`analysis.py` generates:
- Accuracy per prompt  
- Hallucination rate per prompt  
- Summary table  
- Final metrics for the paper  

### âœ” **Visualization**
`visualize.py` creates bar charts for:
- Accuracy  
- Hallucination rate  

These visualizations appear in your paper.

---

## ğŸ“¦ Installation

### 1. Clone the project
git clone https://github.com/<your-repo>/paper-2
cd paper-2

shell
Copy code

### 2. Create and activate virtual environment
python -m venv venv
venv\Scripts\activate # Windows

shell
Copy code

### 3. Install dependencies
pip install -r requirements.txt

yaml
Copy code

---

## â–¶ï¸ Running the Experiments

Run all prompt types:

python run_experiment.py

css
Copy code

All results will be stored in:

results/

yaml
Copy code

---

## ğŸ“ˆ Generate Analysis

python analysis.py

yaml
Copy code

Outputs:

- `analysis/summary.txt`
- `analysis/prompt_performance.csv`

---

## ğŸ“Š Generate Charts

python visualize.py

css
Copy code

Charts saved to:

analysis/charts/

yaml
Copy code

---

## ğŸ“ Paper Assembly

Your paper sections (Markdown files) are located in:

paper/sections/

yaml
Copy code

These can be merged into a final PDF using LaTeX, Typst, or Markdown â†’ PDF export.

---

## ğŸ§  Key Findings (High-Level)

- Instruction-Heavy and Few-Shot prompting yield the highest accuracy  
- Safety prompts minimize hallucination  
- CoT increases reasoning depth but reduces answer stability  
- Self-Verification offers the best safetyâ€“accuracy tradeoff  
- Lightweight models can be effective with proper prompting  

---

## ğŸ“š Citation

If you reuse this work, please cite your final conference paper.

---

## ğŸ¤ Contributors

- **Mohan â€” Lead Researcher & Developer**  
- **ChatGPT â€” Technical assistant for pipeline, prompts, and paper generation**

---

## ğŸ“„ License

This project may be shared for academic and research purposes.